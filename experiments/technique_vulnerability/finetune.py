import os
import warnings
warnings.filterwarnings('ignore')

import argparse
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, multilabel_confusion_matrix
import numpy as np

from utils import *
from load_data import LoadManipDataset
from model_llama import LlamaModel
from model_roberta import RoBERTaModel


def convert_tech_to_list(techniques):
    tech_list = np.zeros(11, dtype=int)
    for tech in techniques:
        if tech == 'Denial':
            tech_list[0] = 1
        elif tech == 'Evasion':
            tech_list[1] = 1
        elif tech == 'Feigning Innocence':
            tech_list[2] = 1
        elif tech == 'Rationalization':
            tech_list[3] = 1
        elif tech == 'Playing the Victim Role' or tech == 'Playing Victim Role':
            tech_list[4] = 1
        elif tech == 'Playing the Servant Role' or tech == 'Playing Servant Role':
            tech_list[5] = 1
        elif tech == 'Shaming or Belittlement':
            tech_list[6] = 1
        elif tech == 'Intimidation':
            tech_list[7] = 1
        elif tech == 'Brandishing Anger':
            tech_list[8] = 1
        elif tech == 'Accusation':
            tech_list[9] = 1
        elif tech == 'Persuasion or Seduction':
            tech_list[10] = 1
    assert sum(tech_list) > 0, f"generated tech_list is all zeros: {techniques}"
    return tech_list


def convert_vul_to_list(vulnerabilities):
    vul_list = np.zeros(5, dtype=int)
    for vul in vulnerabilities:
        if vul == 'Naivete':
            vul_list[0] = 1
        elif vul == 'Dependency':
            vul_list[1] = 1
        elif vul == 'Over-responsibility':
            vul_list[2] = 1
        elif vul == 'Over-intellectualization':
            vul_list[3] = 1
        elif vul == 'Low self-esteem':
            vul_list[4] = 1
    assert sum(vul_list) > 0, f"generated vul_list is all zeros: {vulnerabilities}"
    return vul_list


def prediction(model, test_data, measure):
    targets = []
    if measure == 'tech':
        for v in test_data['Technique'].values:
            techniques = v.split(',')
            targets.append(techniques)
    else:
        for v in test_data['Vulnerability'].values:
            vulnerabilities = v.split(',')
            targets.append(vulnerabilities)

    preds = []
    count = 0
    for idx, row in test_data.iterrows():
        count += 1
        logging.info(f"-----Running {model.model_id} zeroshot prompting ({count}/{len(test_data)})-----")
        dialogue = row['Dialogue']
        pred = model.zeroshot_prompting(dialogue, measure)
        preds.append(pred)

    corrupted_result = 0
    processed_preds, processed_targets = [], []
    for pred, target in zip(preds, targets):
        if len(pred) == 0:
            corrupted_result += 1
        else:
            if measure == 'tech':
                pred = convert_tech_to_list(pred)
                target = convert_tech_to_list(target)
            else:
                pred = convert_vul_to_list(pred)
                target = convert_vul_to_list(target)
            processed_preds.append(pred)
            processed_targets.append(target)

    logging.info(f"\n----------{model.model_id} finetuning result----------")
    logging.info(
        f"Out of {len(preds)} test samples, corrupted samples: {corrupted_result}, processed samples: {len(processed_preds)}")

    # Calculate metrics
    precision = precision_score(processed_targets, processed_preds, average='micro', zero_division=0)
    recall = recall_score(processed_targets, processed_preds, average='micro', zero_division=0)
    micro_f1 = f1_score(processed_targets, processed_preds, average='micro', zero_division=0)
    macro_f1 = f1_score(processed_targets, processed_preds, average='macro', zero_division=0)
    accuracy = accuracy_score(processed_targets, processed_preds)
    conf_matrix = multilabel_confusion_matrix(processed_targets, processed_preds)

    # Print results
    logging.info(f"- Precision = {precision:.3f}")
    logging.info(f"- Recall = {recall:.3f}")
    logging.info(f"- Accuracy = {accuracy:.3f}")
    logging.info(f"- Micro F1-Score = {micro_f1:.3f}")
    logging.info(f"- Macro F1-Score = {macro_f1:.3f}")
    logging.info(f"- Confusion Matrix = \n{conf_matrix}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='finetune-tech-vul')
    parser.add_argument('--model', default='llama-7b', type=str)
    parser.add_argument('--mode', default='train', type=str)   # 'train', 'eval'
    parser.add_argument('--temp', default=0.1, type=float)
    parser.add_argument('--top_p', default=0.5, type=float)
    parser.add_argument('--penal', default=0.0, type=float)
    parser.add_argument('--log_dir', default='./logs', type=str)
    parser.add_argument('--epoch', default=5, type=int)
    parser.add_argument('--train_batch_size', default=8, type=int)
    parser.add_argument('--valid_batch_size', default=8, type=int)
    parser.add_argument('--lr', default=0.0001, type=float)
    parser.add_argument('--measure', default='tech', type=str)
    parser.add_argument('--train_data', default='mentalmanip', type=str)
    parser.add_argument('--eval_data', default='mentalmanip_con', type=str)
    args = parser.parse_args()

    if os.path.exists(args.log_dir) is False:
        os.makedirs(args.log_dir)

    set_logging(args, parser.description)
    show_args(args)

    manip_dataset = LoadManipDataset(file_name=f'../datasets/{args.eval_data}.csv',
                                     train_ratio=0.6,
                                     valid_ratio=0.2,
                                     test_ratio=0.2,
                                     measure=args.measure)

    train_data = manip_dataset.df_train
    valid_data = manip_dataset.df_valid
    test_data = manip_dataset.df_test

    logging.info(f"-----Finetuning Data Size Information-----")
    logging.info(f"Train size: {len(train_data)}")
    logging.info(f"Valid size: {len(valid_data)}")
    logging.info(f"Test size: {len(test_data)}")
    logging.info("")

    if 'llama' in args.model:
        llama_model = "Llama-2-7b-chat-hf"
        if '13b' in args.model:
            llama_model = "Llama-2-13b-chat-hf"

        if args.mode == 'train':
            modelLlama = LlamaModel(load_from_local=False,
                                    model=llama_model,
                                    temperature=0.6,
                                    top_p=0.9,
                                    top_k=50,
                                    repetition_penalty=1.2,
                                    max_new_tokens=1024,
                                    max_input_token_length=4096,
                                    ft_output_dir='llama_ft/'+args.train_data)
            modelLlama.finetuning(train_data,
                                  valid_data,
                                  test_data,
                                  epochs=args.epoch,
                                  train_batch_size=args.train_batch_size,
                                  lr=args.lr,
                                  measure=args.measure)
        elif args.mode == 'eval':
            modelLlama2 = LlamaModel(load_from_local=True,
                                    model='llama_ft/'+args.train_data,
                                    temperature=0.6,
                                    top_p=0.9,
                                    top_k=50,
                                    repetition_penalty=1.2,
                                    max_new_tokens=1024,
                                    max_input_token_length=4096,
                                    ft_output_dir='llama_ft/'+args.train_data)
            prediction(modelLlama2, test_data, args.measure)
        else:
            raise ValueError(f"Invalid mode: {args.mode}")

    elif 'roberta' in args.model:
        roberta_model = "roberta-base"
        if 'large' in args.model:
            roberta_model = "roberta-large"

        if args.mode == 'train' or args.mode == 'eval':
            modelRoBERTa = RoBERTaModel(model=roberta_model,
                                        max_length=512,
                                        train_batch_size=args.train_batch_size,
                                        valid_batch_size=args.valid_batch_size,
                                        epochs=args.epoch,
                                        learning_rate=args.lr,
                                        output_dir='roberta_ft/'+args.train_data,
                                        num_labels=11 if args.measure == 'tech' else 5)
            modelRoBERTa.finetuning(train_data, valid_data, test_data, args.measure)
        else:
            raise ValueError(f"Invalid mode: {args.mode}")