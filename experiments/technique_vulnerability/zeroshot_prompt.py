import os
import warnings

import numpy as np

warnings.filterwarnings('ignore')

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, multilabel_confusion_matrix
import argparse

from load_data import LoadManipDataset
from model_chatgpt import ChatGPTModel
from model_llama import LlamaModel
from utils import *


def convert_tech_to_list(techniques):
    tech_list = np.zeros(11, dtype=int)
    for tech in techniques:
        if tech == 'Denial':
            tech_list[0] = 1
        elif tech == 'Evasion':
            tech_list[1] = 1
        elif tech == 'Feigning Innocence':
            tech_list[2] = 1
        elif tech == 'Rationalization':
            tech_list[3] = 1
        elif tech == 'Playing the Victim Role' or tech == 'Playing Victim Role':
            tech_list[4] = 1
        elif tech == 'Playing the Servant Role' or tech == 'Playing Servant Role':
            tech_list[5] = 1
        elif tech == 'Shaming or Belittlement':
            tech_list[6] = 1
        elif tech == 'Intimidation':
            tech_list[7] = 1
        elif tech == 'Brandishing Anger':
            tech_list[8] = 1
        elif tech == 'Accusation':
            tech_list[9] = 1
        elif tech == 'Persuasion or Seduction':
            tech_list[10] = 1
    assert sum(tech_list) > 0, f"generated tech_list is all zeros: {techniques}"
    return tech_list


def convert_vul_to_list(vulnerabilities):
    vul_list = np.zeros(5, dtype=int)
    for vul in vulnerabilities:
        if vul == 'Naivete':
            vul_list[0] = 1
        elif vul == 'Dependency':
            vul_list[1] = 1
        elif vul == 'Over-responsibility':
            vul_list[2] = 1
        elif vul == 'Over-intellectualization':
            vul_list[3] = 1
        elif vul == 'Low self-esteem':
            vul_list[4] = 1
    assert sum(vul_list) > 0, f"generated vul_list is all zeros: {vulnerabilities}"
    return vul_list


def prediction(model, test_data, measure):
    targets = []
    if measure == 'tech':
        for v in test_data['Technique'].values:
            techniques = v.split(',')
            targets.append(techniques)
    else:
        for v in test_data['Vulnerability'].values:
            vulnerabilities = v.split(',')
            targets.append(vulnerabilities)

    preds = []
    count = 0
    for idx, row in test_data.iterrows():
        count += 1
        logging.info(f"-----Running {model.model_id} zeroshot prompting ({count}/{len(test_data)})-----")
        dialogue = row['Dialogue']
        pred = model.zeroshot_prompting(dialogue, measure)
        preds.append(pred)

    corrupted_result = 0
    processed_preds, processed_targets = [], []
    for pred, target in zip(preds, targets):
        if len(pred) == 0:
            corrupted_result += 1
        else:
            if measure == 'tech':
                pred = convert_tech_to_list(pred)
                target = convert_tech_to_list(target)
            else:
                pred = convert_vul_to_list(pred)
                target = convert_vul_to_list(target)
            processed_preds.append(pred)
            processed_targets.append(target)

    logging.info(f"\n----------{model.model_id} zero prompting result----------")
    logging.info(
        f"Out of {len(preds)} test samples, corrupted samples: {corrupted_result}, processed samples: {len(processed_preds)}")

    # Calculate metrics
    precision = precision_score(processed_targets, processed_preds, average='micro', zero_division=0)
    recall = recall_score(processed_targets, processed_preds, average='micro', zero_division=0)
    micro_f1 = f1_score(processed_targets, processed_preds, average='micro', zero_division=0)
    macro_f1 = f1_score(processed_targets, processed_preds, average='macro', zero_division=0)
    accuracy = accuracy_score(processed_targets, processed_preds)

    conf_matrix = multilabel_confusion_matrix(processed_targets, processed_preds)

    recall_each = recall_score(processed_targets, processed_preds, average=None)
    precision_each = precision_score(processed_targets, processed_preds, average=None)
    logging.info(f"Recall each: {recall_each}")
    logging.info(f"Precision each: {precision_each}\n")

    # Print results
    logging.info(f"- Precision = {precision:.3f}")
    logging.info(f"- Recall = {recall:.3f}")
    logging.info(f"- Accuracy = {accuracy:.3f}")
    logging.info(f"- Micro F1-Score = {micro_f1:.3f}")
    logging.info(f"- Macro F1-Score = {macro_f1:.3f}")
    logging.info(f"- Confusion Matrix = \n{conf_matrix}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='zeroshot-tech-vul')
    parser.add_argument('--model', default='chatgpt', type=str)
    parser.add_argument('--temp', default=0.1, type=float)
    parser.add_argument('--top_p', default=0.5, type=float)
    parser.add_argument('--penal', default=0.0, type=float)
    parser.add_argument('--log_dir', default='./logs', type=str)
    parser.add_argument('--measure', default='tech', type=str)
    parser.add_argument('--data', default='../datasets/mentalmanip_con.csv', type=str)
    args = parser.parse_args()

    if os.path.exists(args.log_dir) is False:
        os.makedirs(args.log_dir)

    set_logging(args, parser.description)
    show_args(args)

    manip_dataset = LoadManipDataset(file_name=args.data,
                                     train_ratio=0.6,
                                     valid_ratio=0.2,
                                     test_ratio=0.2,
                                     measure=args.measure,
                                     split_draw=False)

    test_data = manip_dataset.df_test

    if args.model == 'chatgpt':
        modelChatgpt = ChatGPTModel(gpt_model="gpt-4-1106-preview",
                                    api_key="",  # fill in your OpenAI API key
                                    temperature=0.1,
                                    top_p=0.5,
                                    penal=0.0,
                                    max_input_token_length=4096)
        prediction(modelChatgpt, test_data, args.measure)

    elif 'llama' in args.model:
        llama_model = "Llama-2-7b-chat-hf"
        if '13b' in args.model:
            llama_model = "Llama-2-13b-chat-hf"

        modelLlama = LlamaModel(load_from_local=False,
                                model=llama_model,
                                temperature=0.6,
                                top_p=0.9,
                                top_k=50,
                                repetition_penalty=1.2,
                                max_new_tokens=1024,
                                max_input_token_length=4096,
                                ft_output_dir='llama_ft')

        prediction(modelLlama, test_data, args.measure)
